# -*- coding: utf-8 -*-
"""Gargi Bhardwaj, Shreya Gupta and Megha Arora EMPLOYEE_ATTRITION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZwQD4J1jsshgpxYAMLb1gJAwWeXgt4XS

# Importing all the necessary libraries and models
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.metrics import plot_confusion_matrix 
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.metrics import classification_report

"""# Pandas DataFrame """

df=pd.read_csv(r"C:\Users\dushy\Downloads\WA_Fn-UseC_-HR-Employee-Attrition (1).csv")

df.head()

"""# data preprossesing"""

df.drop(0,inplace=True)
df.isnull().sum()

df.dropna(axis=0,inplace=True)

"""# Feature Engineering

# One hot encoding
"""

def onehot_encode(df, column):
    df = df.copy()
    dummies = pd.get_dummies(df[column], prefix=column)
    df = pd.concat([df, dummies], axis=1)
    df = df.drop(column, axis=1)
    return df

df = df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=1)

df['Gender'] = df['Gender'].replace({'Female': 0, 'Male': 1})
df['OverTime'] = df['OverTime'].replace({'No': 0, 'Yes': 1})
    
    # Ordinal-encode the BusinessTravel column
df['BusinessTravel'] = df['BusinessTravel'].replace({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2})

for column in ['Department', 'EducationField', 'JobRole', 'MaritalStatus']:
        df = onehot_encode(df, column=column)

attrition_dict = df["Attrition"].value_counts()
attrition_dict

sns.set_style('darkgrid')
sns.countplot(x ='Attrition', data = df)

sns.lmplot(x = 'Age', y = 'DailyRate', hue = 'Attrition', data = df)

plt.figure(figsize =(10, 6))
sns.boxplot(y ='MonthlyIncome', x ='Attrition', data = df)

"""# Models used:"""

lr=LogisticRegression(C = 0.1, random_state = 42, solver = 'liblinear')
dt=DecisionTreeClassifier()
rm=RandomForestClassifier()
gnb=GaussianNB()
knn = KNeighborsClassifier(n_neighbors=3)
svm = svm.SVC(kernel='linear')

"""# Traning And Testing Data Split"""

y = df['Attrition']
X = df.drop('Attrition', axis=1)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=4)
X1=X_train
X_train.head()

"""# Fitting Various Models With Train And Test Data"""

for a,b in zip([lr,dt,knn,svm,rm,gnb],["Logistic Regression","Decision Tree","KNN","SVM","Random Forest","Naive Bayes"]):
    a.fit(X_train,y_train)
    prediction=a.predict(X_train)
    y_pred=a.predict(X_test)
    score1=accuracy_score(y_train,prediction)
    score=accuracy_score(y_test,y_pred)
    msg1="[%s] training data accuracy is : %f" % (b,score1)
    msg2="[%s] test data accuracy is : %f" % (b,score)
    print(msg1)
    print(msg2)

"""# Highest Accuracy in Training Data: Random Forest

# 2nd Highest Accuracy : Logistic Regreesion

# 3rd Highest Accuracy: KNN
"""

model_scores={'Logistic Regression':lr.score(X_test,y_test),
             'KNN classifier':knn.score(X_test,y_test),
             'Support Vector Machine':svm.score(X_test,y_test),
             'Random forest':rm.score(X_test,y_test),
              'Decision tree':dt.score(X_test,y_test),
              'Naive Bayes':gnb.score(X_test,y_test)
             }
model_scores

"""# Camparing Different Models Accuracy"""

model_compare=pd.DataFrame(model_scores,index=['accuracy'])
model_compare

"""# RANDOM FOREST :classification_report"""

from sklearn.metrics import classification_report

rm_y_preds = rm.predict(X_test)

print(classification_report(y_test,rm_y_preds))

"""# LOGISTIC REGRESSION: classification_report"""

from sklearn.metrics import classification_report

lr_y_preds = lr.predict(X_test)

print(classification_report(y_test,lr_y_preds))

"""# KNN : classification_report """

from sklearn.metrics import classification_report

knn_y_preds = knn.predict(X_test)

print(classification_report(y_test,knn_y_preds))

"""# Bar Plot MODEL VS ACCURACY"""

model_compare.T.plot(kind='bar') # (T is here for transpose)

# Logistic regression
feature_dict=dict(zip(df.columns,list(lr.coef_[0])))
feature_dict

# loading dependency
import joblib

# saving our model - model - model , filename - model_lr
joblib.dump(lr , 'model_lr')

# opening the file- model_jlib
m_jlib = joblib.load('model_lr')

# check prediction
m_jlib.predict(X_test) # similar output

"""# confusion_matrix """

from sklearn.metrics import plot_confusion_matrix

"""# RANDOM FOREST"""

rm.score(X_train,y_train)
rm.fit(X_train,y_train)
disp=plot_confusion_matrix(rm,X_train,y_train,cmap="Reds",values_format='3g')

"""# LOGISTIC REGRESSION"""

lr=LogisticRegression()
lr.fit(X_train,y_train)
disp=plot_confusion_matrix(lr,X_train,y_train,cmap="Blues",values_format='3g')

"""# KNN"""

knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train,y_train)
disp=plot_confusion_matrix(knn,X_train,y_train,cmap="Greens",values_format='3g')

"""# BAR PLOT : F1 SCORE & ACCURACY FOR DIFFERENT MODELS"""

d={"f1_score":[33,48,24],"accuracy":[86,88,80]}

from pandas import DataFrame
frame=DataFrame(d,index=["random_forest","Logistic Regression","KNN classifier"])

frame

frame.plot(kind='bar')
plt.show()

"""# According to graph best model is Logistic Regression

# WHY LOGISTIC REGRESSION ?

since we can see from the graph the F1 score for "yes" is 33% for random forest and 48% for logistic regression and Accuracy for random forest is 86.05% and for logistic regression is 88% but recall of both is 99% 
so best model is Logistic regression .
"""

import pickle

# Save the trained model as a pickle string.
saved_model = pickle.dumps(lr)

# Load the pickled model
lr_from_pickle = pickle.loads(saved_model)

# Use the loaded pickled model to make predictions
lr_from_pickle.predict(X_test)

import pickle

# Save the trained model as a pickle string.
saved_model = pickle.dumps(rm)

# Load the pickled model
rm_from_pickle = pickle.loads(saved_model)

# Use the loaded pickled model to make predictions
rm_from_pickle.predict(X_test)

"""# Three examples to show accuracy of pridictions of employee attrition

# ex1:
"""

employee1=[[28,0,2,791,1,4,44,3,1,3,2154,6842,0,1,1,1,3,3,0,5,2,2,4,2,0,2,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1]]
print(lr.predict_proba(employee1))

"""# ex 2: """

employee2=[[40,1,2,791,1,4,44,3,1,3,3094,2342,0,1,1,1,5,3,1,5,2,2,4,2,0,2,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1]]
print(rm.predict_proba(employee2))

"""# ex 3:"""

employee3=[[54,1,3,745,1,4,34,3,1,5,3454,6347,0,1,1,1,5,3,1,5,2,2,4,2,0,2,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1]]
print(knn.predict_proba(employee3))

